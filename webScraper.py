import streamlit as st, re, ssl, json, time, google.generativeai as genai
from urllib.request import Request, urlopen
from urllib.parse import urljoin, urlparse
from html.parser import HTMLParser
from collections import Counter
from datetime import datetime
import pandas as pd, plotly.express as px


# ---- Variable 1: G (Global config) ----
#TO WHOM IT MAY CONCERN, DONT WASTE YOUR TIME USING THIS KEY, I'VE ALREADY DELETED IT
#IF YOU ARE A JUDGE THEN PLEASE PUT YOUR GEMINI KEY BELOW ON LINE 13
G = ["AIzaSyCs_UKZ0x68_qIM1LPSkJ9r6we38fUh2u0", {'this', 'that', 'with', 'from', 'have', 'will', 'your', 'about', 'which', 'their', 'there'}]

genai.configure(api_key=G[0])
st.set_page_config(page_title="ğŸŒ Smart Web Crawler", layout="wide")
st.title("ğŸ” Web Crawler & AI Analyzer")
st.caption("Crawl any website, extract info, analyze keywords, and generate an AI summary.")


# ---- Parser ----
class Parser(HTMLParser):
    def __init__(P):
        super().__init__()
        P.d = {'links': [], 'headings': [], 'images': [], 'text': [], 'emails': [], 'phones': []}
        P.t = None

    def handle_starttag(P, t, a):
        a = dict(a)
        if t == 'a' and 'href' in a:
            P.d['links'].append(a['href'])
        if t == 'img' and 'src' in a:
            P.d['images'].append({'src': a['src'], 'alt': a.get('alt', '')})
        if t in ('h1', 'h2', 'h3', 'h4'):
            P.t = t

    def handle_endtag(P, t):
        if t in ('h1', 'h2', 'h3', 'h4'):
            P.t = None

    def handle_data(P, x):
        x = x.strip()
        if not x:
            return
        if P.t:
            P.d['headings'].append({'level': P.t, 'text': x})
        P.d['text'].append(x)
        P.d['emails'] += re.findall(r'\b[\w\.-]+@[\w\.-]+\.\w+\b', x)
        P.d['phones'] += re.findall(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', x)


# Variable 2: X (Used across multiple functions)
def scrape(X):
    try:
        return urlopen(Request(X, headers={"User-Agent": "Mozilla/5.0"}), context=ssl._create_unverified_context(), timeout=10).read().decode('utf-8', 'ignore')
    except:
        return ""


def norm(X, Z):
    if not X or X.startswith(('#', 'js', 'mail', 'tel')):
        return None
    return f"{urlparse(urljoin(Z, X)).scheme}://{urlparse(urljoin(Z, X)).netloc}{urlparse(urljoin(Z, X)).path}".rstrip('/')


# Variable 3: Z (Used across multiple functions)
def crawl(Z, D, F):
    Z = [set(), [Z], {}, urlparse(Z).netloc, None, None, None]
    while Z[1] and len(Z[0]) < D:
        Z[4] = Z[1].pop(0)
        if Z[4] in Z[0]:
            continue
        try:
            F(f"ğŸ” Crawling {len(Z[0]) + 1}/{D}: {Z[4]}")
            Z[5] = Parser();Z[5].feed(scrape(Z[4]))
            Z[2][Z[4]] = {
                'status': 'ok', 'data': Z[5].d, 'links': len(Z[5].d['links']),
                'images': len(Z[5].d['images']), 'headings': len(Z[5].d['headings']),
                'emails': list(set(Z[5].d['emails'])), 'phones': list(set(Z[5].d['phones'])),
                'text_len': sum(len(x) for x in Z[5].d['text'])
            }
            for Z[6] in Z[5].d['links']:
                Z[6] = norm(Z[6], Z[4])
                if Z[6] and urlparse(Z[6]).netloc == Z[3] and Z[6] not in Z[0]:
                    Z[1].append(Z[6])
            Z[0].add(Z[4])
            time.sleep(0.5)
        except Exception as X:
            Z[2][Z[4]] = {'status': 'fail', 'err': str(X)}
            Z[0].add(Z[4])
    return Z[2], Z[0]


# Variable 4: D (Used across multiple functions)
def aggr(D):
    X = {'links': [], 'headings': [], 'images': [], 'text': [], 'emails': [], 'phones': []}
    for Z in D.values():
        if Z.get('status') == 'ok':
            for _ in X:
                X[_].extend(Z['data'][_])
    return X


def stats(D):
    X = [' '.join(D['text']), None, None]
    X[1] = re.findall(r'\b[a-zA-Z]{4,}\b', X[0].lower())
    X[2] = [_ for _ in X[1] if _ not in G[1]]
    return {
        'tw': len(X[1]), 'uw': len(set(X[1])), 'top': Counter(X[2]).most_common(15),
        'em': len(set(D['emails'])), 'ph': len(set(D['phones']))
    }


# Variable 5: F (Used in summary and save)
def summary(F):
    try:
        F = [F[:2000] + "â€¦" if len(F) > 2000 else F, genai.GenerativeModel("gemini-2.0-flash")]
        return F[1].generate_content(f"Summarize the following website content clearly and concisely:\n\n{F[0]}").text.strip()
    except Exception as X:
        return f"{X}"


def save(F, X, D, Z):
    F = [
        f"scrape_{re.sub(r'[^a-zA-Z0-9]', '_', F[:20])}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        {
            'meta': {'url': F, 'time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),'ok': len([_ for _ in X.values() if _.get('status') == 'ok']),'fail': len([_ for _ in X.values() if _.get('status') == 'fail'])},
            'stats': {'links': len(D['links']), 'images': len(D['images']),'words': Z['tw'], 'unique': Z['uw'], 'emails': Z['em'], 'phones': Z['ph']},
            'keywords': Z['top'], 'emails': list(set(D['emails'])), 'phones': list(set(D['phones']))
        }
    ]
    with open(F[0], 'w', encoding='utf-8') as _:
        _.write(json.dumps(F[1], indent=2))
    return F[0]


# ---- Variable 6: U (Main UI state) ----
U = [st.text_input("ğŸŒ Enter site URL"), st.slider("ğŸ“„ Max pages", 1, 50, 5)]

# Initialize session state
if 'crawl_data' not in st.session_state:
    st.session_state.crawl_data = None

if st.button("ğŸš€ Crawl"):
    if not U[0]:
        st.error("Enter valid URL")
    else:
        if not U[0].startswith('http'):
            U[0] = 'https://' + U[0]
        U.extend([st.empty(), None, None, None, None, None])
        
        with st.spinner("Crawling..."):
            U[3] = crawl(U[0], U[1], lambda X: U[2].info(X))
        
        U[4] = aggr(U[3][0])
        U[5] = stats(U[4])
        
        if U[5]['tw'] == 0:
            st.error("âŒ No data found! Please enter a valid URL.")
            st.session_state.crawl_data = None
        else:
            st.success(f"âœ… Done! {len(U[3][1])} pages.");U[6] = save(U[0], U[3][0], U[4], U[5]);st.session_state.crawl_data = {'results': U[3], 'aggr': U[4], 'stats': U[5], 'file': U[6]}

# Display results if data exists in session state
if st.session_state.crawl_data:
    U = [None, None, None, 
         st.session_state.crawl_data['results'],st.session_state.crawl_data['aggr'],st.session_state.crawl_data['stats'],st.session_state.crawl_data['file'],st.columns(4)]  # Directly assign columns to U[7]
    
    st.subheader("ğŸ“Š Stats")
    U[7][0].metric("Words", f"{U[5]['tw']:,}");U[7][1].metric("Unique", f"{U[5]['uw']:,}");U[7][2].metric("Emails", U[5]['em']);U[7][3].metric("Phones", U[5]['ph'])

    st.subheader("ğŸ“ AI Summary")
    st.write(summary(' '.join(U[4]['text'])))

    st.subheader("ğŸ”‘ Top Keywords")
    st.plotly_chart(px.bar(pd.DataFrame(U[5]['top'], columns=['Word', 'Count']),
                           x='Word', y='Count', color='Count', title="Top Keywords"),
                    use_container_width=True)

    # Removed V variable - read file inline
    with open(U[6], 'r', encoding='utf-8') as _:
        st.download_button("ğŸ’¾ Download Report", _.read(), file_name=U[6], mime="application/json")

    with st.expander("ğŸ“§ Emails"):
        st.write(list(set(U[4]['emails'])) or "None")

    with st.expander("ğŸ“± Phones"):
        st.write(list(set(U[4]['phones'])) or "None")

    st.header("ğŸ—‚ï¸ Pages")
    # Reuse _ for both loop variables
    for _ in U[3][0].items():
        if _[1].get('status') == 'ok':
            st.markdown(f"âœ… **{_[0]}** â€” {_[1]['links']} links, {_[1]['images']} imgs, {_[1]['headings']} headings")
            with st.expander(f"ğŸ”— Links in {_[0]}"):
                st.write(_[1]['data']['links'] or "None")
            with st.expander(f"ğŸ“‘ Headings in {_[0]}"):
                st.write(_[1]['data']['headings'] or "None")
        else:
            st.markdown(f"âŒ **{_[0]}** â€” {_[1].get('err')}")

